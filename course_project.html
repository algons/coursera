<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Course Project</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: rgb(88, 72, 246)
   }

   pre .number {
     color: rgb(0, 0, 205);
   }

   pre .comment {
     color: rgb(76, 136, 107);
   }

   pre .keyword {
     color: rgb(0, 0, 255);
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: rgb(3, 106, 7);
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>




</head>

<body>
<h1>Course Project</h1>

<h3><strong>A) Data Stage - Preprocessing</strong></h3>

<p>We first load the libraries and the files and check the training data&#39;s summary information.</p>

<pre><code class="r"># Load libraries
library(&quot;ggplot2&quot;)
library(&quot;caret&quot;)
library(&quot;parallel&quot;)
library(&quot;foreach&quot;)
library(&quot;doParallel&quot;)
library(&quot;knitr&quot;)
library(&quot;randomForest&quot;)
# Load the files
pml.training &lt;- read.table(&quot;~/Documents/study/pml-training.csv&quot;, header = T, 
    sep = &quot;,&quot;)
pml.testing &lt;- read.table(&quot;~/Documents/study/pml-testing.csv&quot;, header = T, sep = &quot;,&quot;)
</code></pre>

<pre><code class="r"># Check summary information
dim(pml.training)
</code></pre>

<pre><code>## [1] 19622   160
</code></pre>

<pre><code class="r">levels(pml.training$user_name)
</code></pre>

<pre><code>## [1] &quot;adelmo&quot;   &quot;carlitos&quot; &quot;charles&quot;  &quot;eurico&quot;   &quot;jeremy&quot;   &quot;pedro&quot;
</code></pre>

<pre><code class="r">class.types &lt;- apply(pml.training, 2, class)
num.nonNA.records &lt;- apply(pml.training, 2, function(x) sum(!is.na(x)))
sum(num.nonNA.records &gt; 15000)
</code></pre>

<pre><code>## [1] 93
</code></pre>

<pre><code class="r">numlevels &lt;- apply(pml.training, 2, function(x) length(unique(x)))
apply(pml.training[, which(numlevels == 2)], 2, function(x) data.frame(table(x))$Freq[2])
</code></pre>

<pre><code>##            new_window     kurtosis_yaw_belt     skewness_yaw_belt 
##                   406                   406                   406 
## kurtosis_yaw_dumbbell skewness_yaw_dumbbell  kurtosis_yaw_forearm 
##                   406                   406                   406 
##  skewness_yaw_forearm 
##                   406
</code></pre>

<p>“pml-training.csv” data includes all data that can be used to build a model. Types of data in this set are;</p>

<ul>
<li> Participant key information: Participant name, exercise timestamps, etc.</li>
<li> Experiment details/measures (used as predictors for model)</li>
<li>  “Classe” variable which is the outcome variable</li>
</ul>

<p>There are several variables with many NA records. It is observed that these variables’ NA records are always in the same row. Therefore, we create an additional variable called “NA.record” to capture missing information.</p>

<p>There are also several variables which have the same NA records mentioned above as “#DIV/0!” and other records as “”. These variables are excluded from the data set since they don’t provide any additional information. Preprocessing codes follow:</p>

<pre><code class="r">numlevels &lt;- apply(pml.training, 2, function(x) length(unique(x)))
NA.records &lt;- which(is.na(pml.training$max_roll_forearm))

# Removing two-level variables
pml.training2 &lt;- pml.training[, -which(numlevels == 2)]

# Converting the variables into numeric - except columns 2 to 5 and
# &#39;classe&#39; variable
pml.training3 &lt;- pml.training2
for (i in c(1, 6:(ncol(pml.training2) - 1))) {
    pml.training3[, i] &lt;- as.numeric(as.character(pml.training2[, i]))
}
# Adding a new variable for the missing cases (&#39;missing&#39; can be also an
# information)
pml.training3$NA.record &lt;- 0
pml.training3$NA.record[NA.records] &lt;- 1
</code></pre>

<h3><strong>B) Sampling</strong></h3>

<p>Data is first randomly split into train and holdout samples, each forming 80% and 20% consecutively. Please note that test samples within the train sample are later used as part of the cross-validation. Each test sample constitutes to 20% of the overall data, while each build sample constitutes to 60%.</p>

<p>Note that some columns had a high number of NAs. These columns are excluded before the modeling can take place.</p>

<pre><code class="r">train &lt;- sample(1:nrow(pml.training3), nrow(pml.training3) * 0.8)
train.data &lt;- pml.training3[train, ]
holdout.data &lt;- pml.training3[-train, ]
</code></pre>

<pre><code class="r"># Remove the columns with too many NAs
NA.threshold.perc &lt;- 0.8  #If 80% of the data is missing, remove
NA.threshold &lt;- nrow(train.data) * NA.threshold.perc
manyNAs &lt;- which(apply(train.data, 2, function(x) sum(is.na(x))) &gt; NA.threshold)

# Approach: Remove the columns with too many NAs, also remove initial
# timestamp etc. columns
columns.to.remove &lt;- c(1:6, manyNAs)

train.clean.data &lt;- train.data[, -columns.to.remove]
train.predictors &lt;- train.clean.data[, -which(colnames(train.clean.data) %in% 
    &quot;classe&quot;)]
train.Y &lt;- train.clean.data$classe
hold.clean.data &lt;- holdout.data[, -columns.to.remove]
hold.predictors &lt;- hold.clean.data[, -which(colnames(hold.clean.data) %in% &quot;classe&quot;)]
hold.Y &lt;- hold.clean.data$classe
</code></pre>

<p>The train and holdout sets are ready. As a final step, outcome flag distribution is checked on both train and holdout sample to make sure the distribution is similar.</p>

<pre><code class="r"># Are classes similarly distributed in train and holdout?
round(table(hold.Y)/length(hold.Y), 2)
</code></pre>

<pre><code>## hold.Y
##    A    B    C    D    E 
## 0.29 0.20 0.17 0.16 0.18
</code></pre>

<pre><code class="r">round(table(train.Y)/length(train.Y), 2)
</code></pre>

<pre><code>## train.Y
##    A    B    C    D    E 
## 0.28 0.19 0.18 0.16 0.18
</code></pre>

<h3><strong>C) Modelling</strong></h3>

<p>Several different models are tried; but to come up with the best model, a cross-validation approach is followed.</p>

<p><em>Cross validation:</em> Training sample is split into 4 folds, each time 3 folds used for model build and 1 fold used to test the model on. “Correctly classified %” on this test sample is used as a measure to choose the best model. This process is repeated 10 times for each model trial, i.e. 40 results are produced for each - where final results are bootstrapped.</p>

<p>Function below is created for the cross validation purposes (details in comments):</p>

<pre><code class="r">MethodTest &lt;- function(X, Y, num.folds, num.runs, method, ncores) {
    # To run the program in parallel processors
    registerDoParallel(cores = ncores)
    # The loop for each run
    foreach.result &lt;- foreach(i = 1:num.runs, .combine = &quot;c&quot;) %dopar% {
        cc.all &lt;- rep(NA, num.folds)
        # Creating k-folds for cross validation
        folds &lt;- createFolds(Y, num.folds)
        # The loop for each fold
        for (j in 1:num.folds) {
            print(paste(&quot;Fold&quot;, j))
            # Create the train and test data sets
            all.folds &lt;- 1:num.folds
            trn.folds &lt;- which(!all.folds %in% j)
            trn.index &lt;- as.numeric(do.call(&quot;c&quot;, folds[trn.folds]))
            tst.index &lt;- folds[[j]]

            trnY &lt;- Y[trn.index]
            tstY &lt;- Y[tst.index]
            trnX &lt;- data.frame(X[trn.index, ])
            tstX &lt;- data.frame(X[tst.index, ])

            # Train the method on train sample
            if (method == &quot;randomForest&quot;) 
                model &lt;- randomForest(x = trnX, y = trnY, ntree = 300) else model &lt;- train(x = trnX, y = trnY, method = method)

            # Predict the test sample results
            prdct &lt;- predict(model, tstX)

            # Find the correctly classified % on test sample
            correctly.classified &lt;- sum(prdct == tstY)/length(tstY)
            cc.all[j] &lt;- correctly.classified
        }
        cc.all
    }
    foreach.result
}
</code></pre>

<p>Following learning algorithms are run.</p>

<ul>
<li>Linear Discriminant Analysis (lda)</li>
<li>  Naïve Bayes (nb)</li>
<li>  Multinomial Regression (multinom)</li>
<li>  Random Forest (rf)</li>
<li>  Also principal components analysis (PCA) done on each method with components explaining 95% of the variance (prin095)</li>
</ul>

<p>5 separate logistic regressions could also be tried; however this would be both computationally expensive and less likely to outperform random forest. Hypothesis here is that since the data set is large and the outcome flag is a 5-category variable, an ensemble of decision trees; more specifically random forest should work the best.</p>

<pre><code class="r">try.lda &lt;- MethodTest(train.predictors, train.Y, 4, 10, method = &quot;lda&quot;, ncores = 1)
try.multinom &lt;- MethodTest(train.predictors, train.Y, 4, 10, method = &quot;multinom&quot;, 
    ncores = 1)
try.rf &lt;- MethodTest(train.predictors, train.Y, 4, 10, method = &quot;randomForest&quot;, 
    ncores = 1)
try.nb &lt;- MethodTest(train.predictors, train.Y, 4, 10, method = &quot;nb&quot;, ncores = 1)

# Now with principal component analysis
train.prin.095 &lt;- preProcess(train.predictors, method = &quot;pca&quot;, thresh = 0.95)
prin095.train &lt;- predict(train.prin.095, train.predictors)
prin095.hold &lt;- predict(train.prin.095, hold.predictors)

try.lda.prin095 &lt;- MethodTest(prin095.train, train.Y, 4, 10, method = &quot;lda&quot;, 
    ncores = 1)
try.multinom.prin095 &lt;- MethodTest(prin095.train, train.Y, 4, 10, method = &quot;multinom&quot;, 
    ncores = 1)
try.rf.prin095 &lt;- MethodTest(prin095.train, train.Y, 4, 10, method = &quot;randomForest&quot;, 
    ncores = 1)
</code></pre>

<p>Each object above includes the correct classification %s on 20 different test samples. Mean of them would give us the average. Now we build the model on whole train sample and calculate the performance on holdout.</p>

<pre><code class="r"># Function that trains the model, predicts on holdout sample and returns
# the correct classification %
HoldoutCC &lt;- function(train.predictors, train.Y, hold.predictors, hold.Y, method) {
    if (method == &quot;rf&quot;) 
        model &lt;- randomForest(x = train.predictors, y = train.Y, ntree = 300) else model &lt;- train(x = train.predictors, y = train.Y, method = method)
    prdct &lt;- predict(model, hold.predictors)
    round(100 * sum(prdct == hold.Y)/length(hold.Y), 1)
}

# Calculate correct classification for each method (including PCA
# approaches)
lda.cc &lt;- HoldoutCC(train.predictors, train.Y, hold.predictors, hold.Y, &quot;lda&quot;)
multinom.cc &lt;- HoldoutCC(train.predictors, train.Y, hold.predictors, hold.Y, 
    &quot;multinom&quot;)
rf.cc &lt;- HoldoutCC(train.predictors, train.Y, hold.predictors, hold.Y, &quot;rf&quot;)
nb.cc &lt;- HoldoutCC(train.predictors, train.Y, hold.predictors, hold.Y, &quot;nb&quot;)
lda.prin095.cc &lt;- HoldoutCC(prin095.train, train.Y, prin095.hold, hold.Y, &quot;lda&quot;)
multinom.prin095.cc &lt;- HoldoutCC(prin095.train, train.Y, prin095.hold, hold.Y, 
    &quot;multinom&quot;)
rf.prin095.cc &lt;- HoldoutCC(prin095.train, train.Y, prin095.hold, hold.Y, &quot;rf&quot;)
</code></pre>

<h3><strong>D) Results</strong></h3>

<p>As mentioned, many results from different runs and folds as part of cross-validation are collected. We expect to see similar results in holdout sample as we see in the bootstrapped cross-validation.</p>

<p>An example to these cross validation tables from which the results will be bootstrapped:</p>

<pre><code class="r">try.lda
</code></pre>

<pre><code>##  [1] 0.6988 0.7036 0.7068 0.6923 0.7075 0.6937 0.6953 0.6984 0.7005 0.6907
## [11] 0.7057 0.7003 0.6925 0.6996 0.7023 0.7067 0.6950 0.7077 0.6915 0.7018
## [21] 0.7052 0.6983 0.6937 0.7044 0.6963 0.6956 0.7080 0.6973 0.7033 0.6953
## [31] 0.6964 0.7030 0.7095 0.6953 0.6962 0.6967 0.6962 0.7057 0.7027 0.7011
</code></pre>

<p>To make the final decision, <em>results table</em> is prepared:</p>

<pre><code class="r">results &lt;- matrix(NA, 7, 4)
colnames(results) &lt;- c(&quot;PCA&quot;, &quot;Method&quot;, &quot;Cross-Valdt cc %&quot;, &quot;Holdout cc%&quot;)
results[, &quot;PCA&quot;] &lt;- c(rep(&quot;No&quot;, 4), rep(&quot;Yes&quot;, 3))
results[1:4, &quot;Method&quot;] &lt;- c(&quot;Linear Discriminant&quot;, &quot;Multinomial Regression&quot;, 
    &quot;Random Forest&quot;, &quot;Naive Bayes&quot;)
results[5:7, &quot;Method&quot;] &lt;- c(&quot;Linear Discriminant&quot;, &quot;Multinomial Regression&quot;, 
    &quot;Random Forest&quot;)

meansum &lt;- function(x) round(100 * mean(x), 1)
results[, 3] &lt;- c(meansum(try.lda), meansum(try.multinom), meansum(try.rf), 
    meansum(try.nb), meansum(try.lda.prin095), meansum(try.multinom.prin095), 
    meansum(try.rf.prin095))
results[, 4] &lt;- c(lda.cc, multinom.cc, rf.cc, nb.cc, lda.prin095.cc, multinom.prin095.cc, 
    rf.prin095.cc)
</code></pre>

<pre><code class="r">results
</code></pre>

<pre><code>##      PCA   Method                   Cross-Valdt cc % Holdout cc%
## [1,] &quot;No&quot;  &quot;Linear Discriminant&quot;    &quot;70&quot;             &quot;71.2&quot;     
## [2,] &quot;No&quot;  &quot;Multinomial Regression&quot; &quot;65.6&quot;           &quot;66&quot;       
## [3,] &quot;No&quot;  &quot;Random Forest&quot;          &quot;99.4&quot;           &quot;99.4&quot;     
## [4,] &quot;No&quot;  &quot;Naive Bayes&quot;            &quot;74.6&quot;           &quot;75.7&quot;     
## [5,] &quot;Yes&quot; &quot;Linear Discriminant&quot;    &quot;52.6&quot;           &quot;53.4&quot;     
## [6,] &quot;Yes&quot; &quot;Multinomial Regression&quot; &quot;52.8&quot;           &quot;53.5&quot;     
## [7,] &quot;Yes&quot; &quot;Random Forest&quot;          &quot;97&quot;             &quot;98&quot;
</code></pre>

<p>*** cc in above table: Correct Classification %</p>

<p>Based on the modelling results, it is clear that random forest method without pre-processing (no PCA) is the best method to differentiate “classe” levels. PCA did not show any improvement in any method. We will use the model that had an aggregated a <strong>99.36%</strong> accuracy on cross validation and <strong>99.44%</strong> accuracy on holdout sample.</p>

<p>This also tells us the error rate we would observe on a completely blind sample would be robust, because holdout sample was not touched at all during the cross validation and model selection. The error rate we’d expect on a separate blind sample would be:  (1 – 0.9944) = <strong>0. 56%</strong></p>

<h3><strong>D) Improvement Areas</strong></h3>

<p>The model can still be improved if more time and resource will be allocated.</p>

<ul>
<li> We can identify the less contributing factors within the RF model via “$importance” table and make a stepwise removal of these factors to see how the model works on the cross validated sample.</li>
<li>  Similar stepwise approach can be used for the multinomial regression as well. The reason it performs the worst among others is because there is high overfitting by including all the variables in the model.</li>
<li>  Overfitting in multinomial model can be also avoided by regularization. However for this, first, the lambda parameter must be optimized.</li>
</ul>

<h3><strong>E) Produce Testing Output</strong></h3>

<p>First, we do the same preprocessing we did on pml_training data.</p>

<pre><code class="r"># Column exclusion and inclusion are not sample dependent We proceed by
# excluding and adding the same columns as in training
for (i in c(6:ncol(pml.testing))) {
    pml.testing[, i] &lt;- as.numeric(as.character(pml.testing[, i]))
}

NA.records &lt;- which(is.na(pml.testing$max_roll_forearm))
pml.testing$NA.record &lt;- 0
pml.testing$NA.record[NA.records] &lt;- 1
</code></pre>

<p>Then we apply our model on pml_testing data.</p>

<pre><code class="r">prdct.testing &lt;- predict(model.rf, pml.testing)
prdct.testing &lt;- as.character(prdct.testing)
</code></pre>

<p>Final output:</p>

<pre><code class="r">prdct.testing
</code></pre>

<pre><code>##  [1] &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;E&quot; &quot;D&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;B&quot; &quot;A&quot; &quot;E&quot; &quot;E&quot; &quot;A&quot;
## [18] &quot;B&quot; &quot;B&quot; &quot;B&quot;
</code></pre>

</body>

</html>

